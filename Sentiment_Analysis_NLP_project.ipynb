{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu4LFTGbNH799w0Q4Dl496",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SfurtiR/Natural-Language-Processing/blob/main/Sentiment_Analysis_NLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Sentiment Analysis NLP project***"
      ],
      "metadata": {
        "id": "JgCrO5gty66X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will:\n",
        "*   Preprocess text using the pipeline.\n",
        "*   Use TextBlob for sentiment analysis.\n",
        "*   Use scikit-learn to train a simple model for sentiment classification.\n"
      ],
      "metadata": {
        "id": "TiRNQ8XxzLN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 :Install and Import Required Libraries**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "imyD8OpzzotW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkvwU1nZ0DLb",
        "outputId": "2fdc740a-f003-4771-bb77-9710fb4260ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.9/590.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m583.7/590.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlkoDIbP0K2y",
        "outputId": "43bdb3ea-df0e-400d-865d-0ed8b7f0f64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import contractions\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ],
      "metadata": {
        "id": "FAyuyX2HzFIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 :Preprocessing Function for Sentiment Analysis**\n"
      ],
      "metadata": {
        "id": "NlTtp0WJ0_K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0L6nPaC0CCV",
        "outputId": "ae9c689d-15b7-46c1-e3ee-7f6dd2db5cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "xYKG6hx40XH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Function to clean and preprocess text data for sentiment analysis.\"\"\"\n",
        "\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Expand contractions (e.g., \"don't\" â†’ \"do not\")\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    # 3. Remove Emojis\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "    # 4. Remove Punctuation & Special Characters\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 5. Tokenization\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # 6. Remove Stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # 7. Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # 8. Spelling Correction\n",
        "    words = [str(TextBlob(word).correct()) for word in words]\n",
        "\n",
        "    return \" \".join(words)  # Return processed text as a string\n"
      ],
      "metadata": {
        "id": "bq2EJ5U30ac2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Load Sentiment Data**\n",
        "\n",
        "\n",
        "\n",
        "Weâ€™ll use a sample dataset with text and sentiment labels (0 for negative, 1 for positive).\n"
      ],
      "metadata": {
        "id": "xs0GDRzr0umK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset (real projects use larger datasets)\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love this product! It's amazing ðŸ˜Š\",\n",
        "        \"This is the worst experience ever. I hate it!\",\n",
        "        \"I'm so happy with my purchase. It's perfect!\",\n",
        "        \"The quality is terrible and I'm very disappointed.\",\n",
        "        \"Absolutely wonderful! Will buy again.\",\n",
        "        \"Not worth the money. Poor quality.\",\n",
        "        \"Great customer service and fast delivery!\",\n",
        "        \"This was a waste of my time and money.\"\n",
        "    ],\n",
        "    \"sentiment\": [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply the preprocessing function\n",
        "df['clean_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEivhjQ20f5y",
        "outputId": "fc913507-f56e-4387-f9b1-49c97c26fc8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  sentiment  \\\n",
            "0                I love this product! It's amazing ðŸ˜Š          1   \n",
            "1      This is the worst experience ever. I hate it!          0   \n",
            "2       I'm so happy with my purchase. It's perfect!          1   \n",
            "3  The quality is terrible and I'm very disappoin...          0   \n",
            "4              Absolutely wonderful! Will buy again.          1   \n",
            "\n",
            "                                        clean_text  \n",
            "0  love product amazing smilingfacewithsmilingeyes  \n",
            "1                       worst experience ever hate  \n",
            "2                           happy purchase perfect  \n",
            "3                    quality terrible disappointed  \n",
            "4                         absolutely wonderful buy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Convert Text Data into Numerical Features**\n",
        "\n",
        "We use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into a numerical format for training."
      ],
      "metadata": {
        "id": "HFCSEn3K1r9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text into numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "kPPP3D2r2L17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Train a Machine Learning Model**\n",
        "\n",
        "Weâ€™ll use Naive Bayes (MultinomialNB), which is great for text classification tasks.\n"
      ],
      "metadata": {
        "id": "nBmA-SKG2XcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "y_pred\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fbqMJ-o2V2Q",
        "outputId": "d8443e3f-57e8-45c7-f53f-803be4707c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67         2\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.50      0.25      0.33         2\n",
            "weighted avg       1.00      0.50      0.67         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Test the Model on New Data**"
      ],
      "metadata": {
        "id": "n33lkkhu2mXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    \"\"\"Function to predict sentiment of a given text\"\"\"\n",
        "    clean_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([clean_text])\n",
        "    prediction = model.predict(text_tfidf)[0]\n",
        "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "# Test with new examples\n",
        "test_texts = [\n",
        "    \"I absolutely love this! It's fantastic.\",\n",
        "    \"Worst product ever. I regret buying it!\",\n",
        "    \"Not bad, but could be better.\",\n",
        "    \"The customer service was very helpful!\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(f\"Text: {text} --> Sentiment: {predict_sentiment(text)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmBjicbu2AJz",
        "outputId": "a0fb045d-1407-4a8c-a453-2b1e5a4641ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I absolutely love this! It's fantastic. --> Sentiment: Positive\n",
            "Text: Worst product ever. I regret buying it! --> Sentiment: Positive\n",
            "Text: Not bad, but could be better. --> Sentiment: Positive\n",
            "Text: The customer service was very helpful! --> Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzw4b75D2KZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}